{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# I wanted to implement a GAN but my derivatives didn't work after a lot of try, so I gave up. If you are available in Monday or Tuesday can we have a look at them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Generative Adverserial Networks\n",
    "\n",
    "In this notebook, I will try to implement the idea of a GAN, for capturing unimodal and bimodal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As our input vector we will always use a uniform random distrubiton, but instead of using between 0-1 we will use 0-100 range to make life a little bit easier for our Generator network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inputVector = np.random.uniform(-100.,100.,(1000000,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Generator Network\n",
    "We will use a basic 2 layer FCN for our generator and discriminator, since our dataset is basically one dimensional it should be enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from cs231n.classifiers.neural_net import * #we will use the 2layer FCN from HW1\n",
    "g_inpsize = 1   #we will just take 1 real number uniform distrubtion\n",
    "g_hidsize = 50 #\n",
    "g_outsize = 1   #we will output a real number from our data distribution\n",
    "\n",
    "Generator = GenNet(g_inpsize, g_hidsize, g_outsize) #it is gonna perform cross entropy loss instead of softmax,\n",
    "                                                         #since our labels are just real or fake, \n",
    "                                                         #it fits our purposes\n",
    "\n",
    "d_inpsize = 1 #first check without batch normaliziton\n",
    "d_hidsize = 50\n",
    "d_outsize = 1 #again we are just gonna output real or fake\n",
    "\n",
    "Discriminator = DiscNet(d_inpsize, d_hidsize, d_outsize) #we will use slightly modified version of TwoLayerNet\n",
    "                                                            #we just added sigmoid at the output layer since all of\n",
    "                                                        #our outputs needs to be in range [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  0.00012329817874\n",
      "dW1 error:  0.000147995532653\n",
      "dW2 error:  0.000190108599713\n",
      "dW3 error:  0.000177618197055\n",
      "db1 error:  0.000186317156775\n",
      "db2 error:  0.000152030529343\n",
      "db3 error:  5.77012881919e-05\n"
     ]
    }
   ],
   "source": [
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "\n",
    "D = 1\n",
    "x = np.random.rand(200, D)\n",
    "y = np.ones((200,1))\n",
    "W1 = np.random.randn(D, d_hidsize)\n",
    "W2 = np.random.randn(d_hidsize, d_hidsize)\n",
    "W3 = np.random.randn(d_hidsize, d_outsize)\n",
    "b1 = np.random.randn(d_hidsize)\n",
    "b2 = np.random.randn(d_hidsize)\n",
    "b3 = np.random.randn(d_outsize)\n",
    "\n",
    "fx = lambda x: Discriminator.loss(x, y)[0]\n",
    "fW1 = lambda W1: Discriminator.loss(x, y, W1=W1, W2=W2, W3=W3, b1=b1, b2=b2, b3=b3)[0]\n",
    "fW2 = lambda W2: Discriminator.loss(x, y, W1=W1, W2=W2, W3=W3, b1=b1, b2=b2, b3=b3)[0]\n",
    "fW3 = lambda W3: Discriminator.loss(x, y, W1=W1, W2=W2, W3=W3, b1=b1, b2=b2, b3=b3)[0]\n",
    "fb1 = lambda b1: Discriminator.loss(x, y, W1=W1, W2=W2, W3=W3, b1=b1, b2=b2, b3=b3)[0]\n",
    "fb2 = lambda b2: Discriminator.loss(x, y, W1=W1, W2=W2, W3=W3, b1=b1, b2=b2, b3=b3)[0]\n",
    "fb3 = lambda b3: Discriminator.loss(x, y, W1=W1, W2=W2, W3=W3, b1=b1, b2=b2, b3=b3)[0]\n",
    "\n",
    "num_grad = lambda x,y: eval_numerical_gradient(x,y, verbose=False, h=1e-6)\n",
    "\n",
    "dx_num = num_grad(fx, x)\n",
    "dW1_num = num_grad(fW1, W1)\n",
    "dW2_num = num_grad(fW2, W2)\n",
    "dW3_num = num_grad(fW3, W3)\n",
    "db1_num = num_grad(fb1, b1)\n",
    "db2_num = num_grad(fb2, b2)\n",
    "db3_num = num_grad(fb3, b3)\n",
    "\n",
    "loss, grads = Discriminator.loss(x, y, W1=W1, W2=W2, W3=W3, b1=b1, b2=b2, b3=b3)\n",
    "dx, dW1, dW2, dW3, db1, db2, db3 = grads['X'], grads['W1'], grads['W2'], grads['W3'], grads['b1'], grads['b2'], grads['b3']\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dW1 error: ', rel_error(dW1_num, dW1)\n",
    "print 'dW2 error: ', rel_error(dW2_num, dW2)\n",
    "print 'dW3 error: ', rel_error(dW3_num, dW3)\n",
    "print 'db1 error: ', rel_error(db1_num, db1)\n",
    "print 'db2 error: ', rel_error(db2_num, db2)\n",
    "print 'db3 error: ', rel_error(db3_num, db3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  4.65945311948e-05\n",
      "dW1 error:  0.000471942352486\n",
      "dW2 error:  0.000705263433703\n",
      "dW3 error:  0.000617220399923\n",
      "db1 error:  0.000709421975209\n",
      "db2 error:  0.000579435417246\n",
      "db3 error:  0.000138053408846\n"
     ]
    }
   ],
   "source": [
    "D = 1\n",
    "x = np.random.randn(200, D)\n",
    "W1 = np.random.randn(D, g_hidsize)\n",
    "W2 = np.random.randn(g_hidsize, g_hidsize)\n",
    "W3 = np.random.randn(g_hidsize, g_outsize)\n",
    "b1 = np.random.randn(g_hidsize)\n",
    "b2 = np.random.randn(g_hidsize)\n",
    "b3 = np.random.randn(g_outsize)\n",
    "\n",
    "fx = lambda x: Generator.loss(x, Discriminator, W1=W1, W2=W2, W3=W3, b1=b1, b2=b2, b3=b3)[0]\n",
    "fW1 = lambda W1: Generator.loss(x, Discriminator, W1=W1, W2=W2, W3=W3, b1=b1, b2=b2, b3=b3)[0]\n",
    "fW2 = lambda W2: Generator.loss(x, Discriminator, W1=W1, W2=W2, W3=W3, b1=b1, b2=b2, b3=b3)[0]\n",
    "fW3 = lambda W3: Generator.loss(x, Discriminator, W1=W1, W2=W2, W3=W3, b1=b1, b2=b2, b3=b3)[0]\n",
    "fb1 = lambda b1: Generator.loss(x, Discriminator, W1=W1, W2=W2, W3=W3, b1=b1, b2=b2, b3=b3)[0]\n",
    "fb2 = lambda b2: Generator.loss(x, Discriminator, W1=W1, W2=W2, W3=W3, b1=b1, b2=b2, b3=b3)[0]\n",
    "fb3 = lambda b3: Generator.loss(x, Discriminator, W1=W1, W2=W2, W3=W3, b1=b1, b2=b2, b3=b3)[0]\n",
    "\n",
    "num_grad = lambda x,y: eval_numerical_gradient(x,y, verbose=False)\n",
    "\n",
    "dx_num = num_grad(fx, x)\n",
    "dW1_num = num_grad(fW1, W1)\n",
    "dW2_num = num_grad(fW2, W2)\n",
    "dW3_num = num_grad(fW3, W3)\n",
    "db1_num = num_grad(fb1, b1)\n",
    "db2_num = num_grad(fb2, b2)\n",
    "db3_num = num_grad(fb3, b3)\n",
    "\n",
    "loss, grads = Generator.loss(x, Discriminator, W1=W1, W2=W2, W3=W3, b1=b1, b2=b2, b3=b3)\n",
    "dx, dW1, dW2, dW3, db1, db2, db3 = grads['X'], grads['W1'], grads['W2'], grads['W3'], grads['b1'], grads['b2'], grads['b3']\n",
    "\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dW1 error: ', rel_error(dW1_num, dW1)\n",
    "print 'dW2 error: ', rel_error(dW2_num, dW2)\n",
    "print 'dW3 error: ', rel_error(dW3_num, dW3)\n",
    "print 'db1 error: ', rel_error(db1_num, db1)\n",
    "print 'db2 error: ', rel_error(db2_num, db2)\n",
    "print 'db3 error: ', rel_error(db3_num, db3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Let us start with a standart distribution as our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "N = 1000000\n",
    "mean = 0.0\n",
    "std = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = np.random.normal(mean, std, (N,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAHVCAYAAADLiU4DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBQAwgOgCABhAdAEADCC6AAAG+P9AWAyezzS2AQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5e46626b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(data, 100, normed=1)\n",
    "_ = plt.hist(inputVector[np.abs(inputVector)<4], 100, normed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0/50000, Discriminator(mean,std): (0.000685,0.000000), Generated: (0.000685,0.000000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cs231n/classifiers/neural_net.py:93: RuntimeWarning: Mean of empty slice.\n",
      "  print 'Iteration %d/%d, Discriminator(mean,std): (%f,%f), Generated: (%f,%f)' % (t, num_its, passed.mean(), passed.std(), gbatch.mean(), gbatch.std())\n",
      "/usr/lib/python2.7/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/lib/python2.7/site-packages/numpy/core/_methods.py:135: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/usr/lib/python2.7/site-packages/numpy/core/_methods.py:105: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/usr/lib/python2.7/site-packages/numpy/core/_methods.py:127: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10000/50000, Discriminator(mean,std): (nan,nan), Generated: (2.759887,0.098506)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cs231n/rnn_layers.py:213: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  pos_mask = (x >= 0)\n",
      "cs231n/rnn_layers.py:214: RuntimeWarning: invalid value encountered in less\n",
      "  neg_mask = (x < 0)\n",
      "cs231n/layers.py:316: RuntimeWarning: divide by zero encountered in divide\n",
      "  dx = -(y/x - (1.-y)/(1.-x))/N\n",
      "cs231n/layers.py:87: RuntimeWarning: invalid value encountered in multiply\n",
      "  dx = x*(1.-x)*dout\n",
      "cs231n/layers.py:77: RuntimeWarning: invalid value encountered in greater\n",
      "  dx = np.where(x > 0, dout, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20000/50000, Discriminator(mean,std): (nan,nan), Generated: (nan,nan)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-036d3e24d1dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m            verbose=True, print_every=10000)\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/kadircet/repos/ceng/783/HW3/cs231n/classifiers/neural_net.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_its\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_every\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kadircet/repos/ceng/783/HW3/cs231n/classifiers/neural_net.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kadircet/repos/ceng/783/HW3/cs231n/classifiers/neural_net.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/home/kadircet/repos/ceng/783/HW3/cs231n/layers.pyc\u001b[0m in \u001b[0;36maffine_backward\u001b[0;34m(dout, cache)\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m   \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m   \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "g_inpsize = 1   #we will just take 1 real number uniform distrubtion\n",
    "g_hidsize = 50 #\n",
    "g_outsize = 1   #we will output a real number from our data distribution\n",
    "\n",
    "Generator = GenNet(g_inpsize, g_hidsize, g_outsize) #it is gonna perform cross entropy loss instead of softmax,\n",
    "                                                         #since our labels are just real or fake, \n",
    "                                                         #it fits our purposes\n",
    "\n",
    "d_inpsize = 1 #first check without batch normaliziton\n",
    "d_hidsize = 50\n",
    "d_outsize = 1 #again we are just gonna output real or fake\n",
    "\n",
    "Discriminator = DiscNet(d_inpsize, d_hidsize, d_outsize) #we will use slightly modified version of TwoLayerNet\n",
    "                                                            #we just added sigmoid at the output layer since all of\n",
    "                                                        #our outputs needs to be in range [0,1]\n",
    "\n",
    "Trainer = GANTrainer(Generator, Discriminator, data, update_rule='adam',\n",
    "           num_epochs=5,\n",
    "           batch_size=100,\n",
    "           optim_config={\n",
    "             'learning_rate': 2e-5,\n",
    "           },\n",
    "           lr_decay=0.995,\n",
    "           verbose=True, print_every=10000)\n",
    "\n",
    "Trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot([x[0] for x in Trainer.loss_history])\n",
    "plt.plot([x[1] for x in Trainer.loss_history])\n",
    "plt.plot([x[2] for x in Trainer.loss_history])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
